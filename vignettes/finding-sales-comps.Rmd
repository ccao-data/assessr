---
title: "Finding sales comparables using CkNN"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Sales comparables are recent sales which have the same (or very similar) characteristics as a target (unsold) property. They are frequently used by assessors, real estate agents, and appraisers to determine the fair value of a home. However, finding comparable properties at scale can be difficult. 

The clustered k-nearest neighbors algorithm from AssessR can be used to quickly find comparables for any number of unsold properties. This vignette demonstrates the process of using the `cknn()` function to find comparable sales in Evanston, Illinois.

## CkNN algorithm

The `cknn()` algorithm accepts a set of sales as input and works in two stages: 

1. Divide the full set of sales into $m$ clusters according to each property's characteristics. This mimics the process of market segmentation or separating properties into different classes. This clustering is done using the k-prototypes algorithm from [clustMixType](https://rjournal.github.io/archive/2018/RJ-2018-048/RJ-2018-048.pdf).
2. For each property $i$, find the $k$ nearest neighbors within $i$'s cluster, minimizing the distance over planar coordinates and Euclidean distance to *all* cluster centers. This is accomplished with the fast kNN function from [dbscan](https://cran.r-project.org/web/packages/dbscan/dbscan.pdf).

The result of this algorithm is a model object containing the cluster membership of each property, as well its $k$ nearest in-cluster neighbors. `cknn()` also has a predict method which can be used to find comparables for properties not in the original dataset.

The remainder of this vignette outlines the process of using `cknn()`, first to create market clusters, then to find sales comparables for a sample of unsold properties.

## Clustering

### Loading the data

To demonstrate `cknn()`, we will load real sales and assessment data from the [Cook County Open Data Portal](https://datacatalog.cookcountyil.gov/Property-Taxation/Cook-County-Assessor-s-Residential-Assessments/uqb9-r7vn). This data can be loaded using a variety of methods, but the easiest way is to use the API to return JSON results, which can be loaded using the `jsonlite` library.

```{r, message=FALSE}
# Load libraries for this project
library(dplyr)
library(ggplot2)
library(sf)
library(jsonlite)
library(assessr)
library(ccao)

# Load 100k rows of characteristic data for Evanston
properties <- read_json(
  "https://datacatalog.cookcountyil.gov/resource/bcnq-qi2z.json?$limit=10000&town_code=17",
  simplifyVector = TRUE
)

# Load max 100k rows of sales data for Evanston
sales <- read_json(
  "https://datacatalog.cookcountyil.gov/resource/5pge-nu6u.json?$limit=10000&town_code=17",
  simplifyVector = TRUE
)
```

### Preparing the data

Like most clustering methods, `cknn()` requires carefully prepared input data to produce good results. The following preparation steps are strongly recommended:

* Categorical variables must be converted to factors. Rare factor types in each variable should be dropped or aggregated into a separate factor level. For example, if only 1% of properties have a type of roof, it may make sense to change the roof type variable of those properties to NA, or to change them to a similar roof type.
* Numeric variables should have their outliers removed, which can be accomplished using the `is_outlier()` function. Scaling numeric variables is not necessary, as `cknn()` will automatically scale all numeric input variables to be between 0 and 1.
* Latitude and longitude inputs *must* be reprojected into a planar projection, preferably one specific to the area of interest. Using Cook County as an example, lat and lon coordinates are reprojected into [EPSG 3435](https://epsg.io/3435), which covers Eastern Illinois.
* The `cknn()` function assumes that **all** variables in the input data are used for clustering. As such, the prepared data should not contain non-clustering variables such as parcel ID. Since `cknn()` preserves the order of the input data, its results can easily be joined back to the original data.

In addition to these steps, the example case below also removes transactions with low-value sales, those prior to 2016, and those with missing values.

```{r}
# Keep only arms length transactions for properties with characteristics
sales_prepped <- sales %>%
  filter(
    sale_year >= 2016,
    sale_price >= 10000,
    !class %in% c(211, 212, 299),
    as.numeric(rooms) <= 14,
    !is.na(centroid_x)
  ) %>%
  # Keep only the variables used for clustering (except sale price, which is
  # removed later)
  select(
    sale_price, bldg_sf, age, rooms, beds,
    air, bsmt, bsmt_fin, ext_wall, heat, gar1_size,
    lon = centroid_x, lat = centroid_y
  ) %>%
  # Convert categorical variables to factor and numbers to numeric
  mutate(
    across(air:gar1_size, as.factor),
    across(c(sale_price:beds, lon, lat), as.numeric)
  ) %>%
  # Convert lat/lon to planar projection. In the case of Illinois, 3435 is ideal
  # This code converts to the new coordinate system, but immediately removes the
  # resulting geometry column (only the coordinates are needed)
  st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
  st_transform(3435) %>%
  mutate(lon = st_coordinates(.)[, 1], lat = st_coordinates(.)[, 2]) %>%
  st_set_geometry(NULL)

# Visualize prepared data
head(sales_prepped) %>%
  select(-sale_price) %>%
  knitr::kable(digits = 3)
```

### Creating clusters

Now that we've prepared the input data, we can use the `cknn()` function to create clusters. The `cknn()` function accepts arbitrary relative weights via the `var_weights` input. These weights can take the following form:

* A named list with names corresponding to column names in the input data (see example below). Names not included in the list are assumed to have a value of 1. These weights are multiplied by the variance estimates created by `clustMixType::lambdaest()`. Higher values will weight variables more heavily in the clustering.
* A $p$ long unnamed vector, where $p$ is equal to the number columns in the input data. These weights are **not** multiplied by the variance estimates created by `clustMixType::lambdaest()`.
* A single unnamed numeric value. This value trades off the relative importance of numeric versus categorical variables. Higher values will more heavily weight categorical variables, while a value of 0 replicates standard k-means (numerics only).
* A `NULL` value. This uses the default estimates produced by `clustMixType::lambdaest()`. All variables are weighted equally.

Since we don't know the optimal number of clusters, we can loop through a vector of possible values for $m$ (number of clusters), and then try to pick the best one.

```{r, message=FALSE, warning=FALSE, results='hide'}
# Set arbitrary relative weights, unspecified vars = 1
weights <- c("bldg_sf" = 8, "age" = 4, "ext_wall" = 2)

# Set the number of potential clusters to try
num_clusts <- 2:15

# Loop through different values of m, outputting the results to a list
clusts <- lapply(num_clusts, function(m) {
  cknn(
    data = sales_prepped %>% select(-sale_price, -lon, -lat),
    lon = sales_prepped %>% pull(lon),
    lat = sales_prepped %>% pull(lat),
    var_weights = weights,
    m = m,
    k = 10,
    l = 0.5
  )
})
```

### Choosing the optimal number of clusters

The optimal number of clusters depends on the input data, knowledge of the market, and trade-offs between accuracy and overfitting. There is no perfect method, but there are a number of heuristics to help make the choice. Here we use the elbow method to determine a suitable number, in this case $m = 8$.

```{r, message=FALSE, out.width='100%', results='hold'}
# Elbow plot of total sum square dists * number of clusters
data.frame(
  m = num_clusts,
  ss = sapply(clusts, function(x) x$kproto$tot.withinss)
) %>%
  ggplot() +
  geom_line(aes(x = m, y = ss), size = 1.2, color = "blue") +
  labs(x = "# of Clusters", y = "Total Sum Square Dists") +
  theme_minimal()

# Choose the best m according to the elbow plot. Clustering starts at 2,
# so m minus 1 is best number of clusters. We're choosing m = 8
best_clust <- clusts[[8 - 1]]
```

### Visualizing the clusters

To visualize whether or not our chosen set of clusters conforms to market expectations, we can visualize each cluster on a map, along with its median sale price, age, and building square footage. Keep in mind that these three variables are not the only ones included in the clustering.

```{r, message=FALSE, out.width='100%'}
# Join the resulting cluster memberships back to the original data frame.
# The m column contains cluster membership for each property, while the knn
# column contains the row index positions of that property's nearest neighbors
sales_with_knn <- sales_prepped %>%
  mutate(
    m = best_clust$kproto$cluster,
    knn = best_clust$knn
  )

# Plot the created clusters, getting the median values for each cluster,
# then mapping each one as an individual facet
sales_with_knn %>%
  group_by(m) %>%
  mutate(
    across(any_of(c("sale_price", "age", "bldg_sf")), as.numeric),
    median_sp = median(sale_price),
    median_age = median(age),
    median_sf = median(bldg_sf)
  ) %>%
  ungroup() %>%
  mutate(
    m = forcats::fct_reorder(factor(m), median_sp),
    median_sp = scales::dollar(median_sp),
    median_age = paste(scales::comma(median_age, accuracy = 1), "years"),
    median_sf = paste(scales::comma(median_sf, accuracy = 1), "sqft.")
  ) %>%
  st_as_sf(coords = c("lon", "lat"), crs = 3435) %>%
  st_transform(4326) %>%
  ggplot() +
  geom_sf(data = st_set_crs(ccao::town_shp, 4326) %>% filter(township_name == "Evanston")) +
  geom_sf(aes(color = m, geometry = geometry), size = 0.9) +
  geom_text(aes(x = -87.748, y = 42.048, label = median_sp), check_overlap = TRUE, hjust = 0) +
  geom_text(aes(x = -87.748, y = 42.038, label = median_age), check_overlap = TRUE, hjust = 0) +
  geom_text(aes(x = -87.748, y = 42.028, label = median_sf), check_overlap = TRUE, hjust = 0) +
  facet_wrap(vars(m), ncol = 4) +
  theme_void() +
  theme(legend.position = "none")
```


## Finding comparables

Now that we've created a `cknn()` object, we can feed it new characteristics data from unsold properties to find sales comparables. The predict method used below will take new data **in exactly the same form as the original data** input to the `cknn()` function. It will then output a cluster memberships and set of $k$ nearest neighbors for each property in the new data. The $k$ nearest neighbors are effectively sales comparables.

### Preparing the new data

Data input for the predict method must be in exactly the same form as the data originally input to the `cknn()` function. In practice, this means the input data must have the same number of columns, column types, and column names as the sales data. Here we've effectively copied the code used to prepare the previous data. Note that the `price` will be excluded later.

```{r prep_comps}
# Keep only properties with characteristics that are NOT in the set of sold
# properties recorded in the sales data frame
properties_prepped <- properties %>%
  filter(!pin %in% sales$pin) %>%
  filter(
    !class %in% c(211, 212, 299),
    rooms <= 14,
    !is.na(centroid_x)
  ) %>%
  mutate(price = as.numeric(pri_est_bldg) + as.numeric(pri_est_land)) %>%
  # Keep exactly the same columns as the sales_prepped data frame
  select(
    price, bldg_sf, age, rooms, beds,
    air, bsmt, bsmt_fin, ext_wall, heat, gar1_size,
    lon = centroid_x, lat = centroid_y
  ) %>%
  # Convert to numeric and factor types
  mutate(
    across(air:gar1_size, as.factor),
    across(c(bldg_sf:beds, lon, lat), as.numeric)
  ) %>%
  # Reproject coordinates into planar meters
  st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
  st_transform(3435) %>%
  mutate(lon = st_coordinates(.)[, 1], lat = st_coordinates(.)[, 2]) %>%
  st_set_geometry(NULL)

# Visualize prepared data
head(properties_prepped) %>%
  select(-price) %>%
  knitr::kable(digits = 3)
```

### Finding comparables using new data

The predict method works using inputs that are nearly identical to those of the `cknn()` function. It outputs a list containing the cluster membership and nearest neighbors for each property in the new data. Here, we've used that list to construct a new data frame containing both the unsold properties **and** the comparable sales from the original data. Note that the predict method takes an argument $k$, which can be used to vary the number of comparables returned. It also takes an argument $l$, which can vary the trade-off between distance and characteristic similarities.

```{r, include=FALSE}
l <- 0.9
```

```{r find_comps}
# Set the number of comps to return. If k is unspecified in predict() the original
# value used to create the cknn object is used
k <- 10

# Find the set of cluster memberships and comparables for each unsold property
comparables <- predict(
  best_clust,
  newdata = properties_prepped %>% select(-price, -lon, -lat),
  lon = properties_prepped %>% pull(lon),
  lat = properties_prepped %>% pull(lat),
  k = k,
  l = l # set to 0.9 here
)

# Use those memberships to construct a dataset of both unsold and sold properties
# The "group" variable here corresponds to each unsold property and its set of
# comparables. The "is_sale" variable is a dummy used to construct the group
comparables_df <- sales_prepped %>%
  mutate(is_sale = 1) %>%
  rename(price = sale_price) %>%
  slice(unlist(comparables$knn)) %>%
  mutate(group = rep(seq_len(nrow(properties_prepped)), times = 1, each = k)) %>%
  bind_rows(
    properties_prepped %>%
      mutate(group = row_number())
  ) %>%
  mutate(type = ifelse(is.na(is_sale), "Target", "Comp")) %>%
  relocate(group:type, .before = "price") %>%
  arrange(group, desc(type)) %>%
  select(-is_sale)
```

### Visualizing different comparable sets {.tabset .tabset-pills}

We can now visualize the resulting comparables in a variety of ways. To see the trade-off caused by changing $l$, we choose a random subset of comparable groups from the dataset, then compare them for $l = 0.9$ and $l = 0.1$.

Each page of the table shown below shows a different comparables set (5 sets in total). Switching between the two tables reveals that $l = 0.1$ yields much more physically similar properties.

However, good comparables should also be nearby the target property. We can check the spatial distribution of our comparables sets by plotting them on a map of Evanston. Note that for $l = 0.1$ the properties are much more sparsely distributed. This may be appropriate for a small area such as Evanston, but might be less acceptable for a larger area such as an entire county. 

Finally, good comparables will be close to each other in price. Here we show distributions of sale prices for each comparables set, along with the last assessed value for each target property. The resulting distributions are quite wide for some properties, however, our clustering was done using just 10 variables. Adding further variables or choosing a higher $m$ value will likely result in tighter distributions.

```{r plots, child="_finding-sales-comps-plots.Rmd"}
```

```{r, include=FALSE}
l <- 0.1
```

```{r find_comps2, include=FALSE, ref.label="find_comps"}
```

```{r plots2, child="_finding-sales-comps-plots.Rmd"}
```

## Suggestions

`cknn()` can be used to effectively find market clusters and sales comparables, but it comes with a number of challenges. Here are some suggestions to get the best performance out of the algorithm:

1. Always clean your input data, both when initially creating clusters and when using the predict method. Due to the nature of `cknn()`'s scaling, it is especially sensitive to numeric outliers.
2. Choose an initial value of $m$ based on the [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) or [silhouette method](https://en.wikipedia.org/wiki/Silhouette_(clustering)). For even better performance, the predict function can be used to tune $m$ based on a validation set. Higher values of $m$ will lead to tighter, smaller clusters, but may lead to overfitting.
3. The relative weights passed to the `var_weights` argument in `cknn()` should be based on some measure of variable importance. Many assessors have an intuitive notion of what variables drive value. If that intuition can be translated to a numeric ranking, then it's possible to use it as a set of weights. For a more empirical method, it's possible to create a set of weights using methods such as random forest or permutation feature importance.
4. Keep $k$ relatively low for tight spatial and price distributions. Increasing the value of $k$ will yield increasingly dissimilar/far away sales. 
5. Choose a value for $l$ based on your subjective need. If you value proximity, keep $l$ close to 1, if you value very similar properties, keep $l$ near 0. Otherwise, the default value (0.5) represents a good trade-off between distance and similarity.
